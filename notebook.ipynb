{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d2edb8",
   "metadata": {},
   "source": [
    "# Assessing the Sentiment of American International Affairs' Experts\n",
    "> Eric GutiÃ©rrez, 10th February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3d89a",
   "metadata": {},
   "source": [
    "### 0. Motivation\n",
    "We want to assess the sentiment of American International Affairs' experts through the years. In order to do so, we use the reports published by the Council on Foreign Relations (CFR) from 1998 to 2026. We obtain the most distinct concepts that appear in a given year's reports. In addition, we also use a dictionary method to generate an index on supply-chain challenges through the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc99cc9",
   "metadata": {},
   "source": [
    "### 1. Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    #options.add_argument(\"--headless=new\") # Uncomment to run invisibly\n",
    "    driver = uc.Chrome(options=options, version_main=144)\n",
    "    driver.set_page_load_timeout(20)\n",
    "    return driver\n",
    "\n",
    "# Initialize Driver\n",
    "driver = init_driver()\n",
    "\n",
    "link = \"https://www.cfr.org/reports\"\n",
    "\n",
    "reports_href = []\n",
    "\n",
    "for page in range(1,35):\n",
    "    try:\n",
    "        driver.get(link+f\"?page={page}\")\n",
    "        print(f'Loading page {page}...')\n",
    "        print(len(reports_href))\n",
    "        cards = driver.find_elements(By.XPATH, \"//a[contains(@class, 'block before:absolute before:inset-0 before:z-1 hover:link-underline')]\")\n",
    "        for rep in cards:\n",
    "            reports_href.append(rep.get_attribute(\"href\"))\n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "reports = []\n",
    "\n",
    "ids = 0\n",
    "\n",
    "for num, url in enumerate(reports_href):\n",
    "    print(f'Retrieving info from url {num}/{len(reports_href)}')\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        content = driver.find_element(By.ID, \"page-content\")\n",
    "        date = driver.find_element(By.XPATH, \"//time[contains(@class, 'mx-auto my-6 text-center type-sans-body7')]\")\n",
    "        reports.append({\"report_id\": ids, \"title\": driver.title, \"date\": date.text, \"body\": content.text})\n",
    "        ids = ids + 1\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "reports_df = pd.DataFrame(reports)\n",
    "\n",
    "reports_df.to_csv('data/cfr_complete_11Feb.csv')\n",
    "print('Reports successfully saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, url in enumerate(reports_href[158:]):\n",
    "    print(f'Retrieving info from url {num}/{len(reports_href)}')\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        content = driver.find_element(By.ID, \"page-content\")\n",
    "        date = driver.find_element(By.XPATH, \"//time[contains(@class, 'mx-auto my-6 text-center type-sans-body7')]\")\n",
    "        reports.append({\"report_id\": ids, \"title\": driver.title, \"date\": date.text, \"body\": content.text})\n",
    "        ids = ids + 1\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "reports_df = pd.DataFrame(reports)\n",
    "\n",
    "reports_df.to_csv('data/cfr_complete_11Feb.csv')\n",
    "print('Reports successfully saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e09262",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/cfr_complete_10Feb.csv', index_col='report_id')\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize(txt):\n",
    "    txt = txt.split(\"ACKNOWLEDGMENTS\")[0]\n",
    "    doc = sp(txt)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_.strip() != '']\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "corpus[\"text_preproc\"] = corpus[\"body\"].astype(str).apply(lambda x: lemmatize(x))\n",
    "corpus['year'] = corpus['date'].apply(lambda x: x.split(\" \")[1])\n",
    "\n",
    "sp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def clean_text(txt):\n",
    "    return str(txt).split(\"ACKNOWLEDGMENTS\")[0]\n",
    "\n",
    "texts = (clean_text(text) for text in corpus[\"body\"])\n",
    "\n",
    "lemmatized_docs = []\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "for doc in sp.pipe(texts, batch_size=50, n_process=1):\n",
    "    lemmas = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_.strip() != ''\n",
    "    ]\n",
    "    lemmatized_docs.append(\" \".join(lemmas))\n",
    "\n",
    "corpus[\"text_preproc\"] = lemmatized_docs\n",
    "corpus['year'] = corpus['date'].apply(lambda x: x.split(\" \")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900d767",
   "metadata": {},
   "source": [
    "### 3. Generating the Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5a6f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Eric\\Desktop\\Arxius\\git\\text_mining_workflow_cfr\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:411: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['association', 'cfr', 'international', 'produce'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Documents: 697\n",
      "# Terms: 37838\n",
      "\n",
      "--- Top Distinctive Words by Year ---\n",
      "1996 (1 documents): sudden, flow develop, import substitution, difference, monetary\n",
      "1997 (2 documents): religion, islam, china, religious, iranian\n",
      "1998 (8 documents): japan, trade, alliance, asian, asia\n",
      "1999 (9 documents): trade, financial, north, space, market\n",
      "2000 (21 documents): colombia, japan, china, defense, islamist\n",
      "2001 (9 documents): afghanistan, food, border, gm, genetically\n",
      "2002 (10 documents): egypt, saudi, egyptian, corporate governance, mena\n",
      "2003 (6 documents): iraq, iraqi, periodical, papua, reform\n",
      "2004 (8 documents): philippines, girl, darfur, aids, georgia\n",
      "2005 (8 documents): america, opinion leader, hiv, iraq, american\n",
      "2006 (13 documents): patent, turkey, trade, nuclear, turkish\n",
      "2007 (17 documents): iraq, africa, bolivia, angola, pakistan\n",
      "2008 (21 documents): education, nuclear, iran, space, assurance\n",
      "2009 (33 documents): financial, imbalance, center geoeconomic studies, center geoeconomic, geoeconomic studies\n",
      "2010 (34 documents): nuclear, eu, nuclear weapon, european, weapon\n",
      "2011 (50 documents): family planning, planning, reserve, family, renminbi\n",
      "2012 (52 documents): korea, south, south korea, nuclear, china\n",
      "2013 (47 documents): oil, debt, korea, south, growth\n",
      "2014 (46 documents): market, debt, bank, subsidy, internet\n",
      "2015 (47 documents): china, korea, currency, market, debt\n",
      "2016 (56 documents): scorecard, growth, china, market, price\n",
      "2017 (51 documents): china, korea, participant, south, russia\n",
      "2018 (32 documents): china, participant, cyber, energy, southeast\n",
      "2019 (22 documents): climate, energy, woman, algeria, taiwan\n",
      "2020 (22 documents): refugee, china, covid 19, covid, 19\n",
      "2021 (17 documents): china, major power, competition, india, nuclear\n",
      "2022 (20 documents): climate, pandemic, health, china, gfa\n",
      "2023 (12 documents): health, climate, global health, climate change, biotechnology\n",
      "2024 (8 documents): russia, ukraine, russian, nato, china\n",
      "2025 (13 documents): ukraine, china, conflict, taiwan, nuclear\n",
      "2026 (2 documents): mineral, american, critical mineral, grand strategy, waste\n"
     ]
    }
   ],
   "source": [
    "my_stop_words = [\n",
    "    'overview', 'content', 'pdf', 'download', 'spanish', 'rights', 'reserved', \n",
    "    'click', 'view', 'paper', 'working', 'series', 'copyright', 'www', 'http',\n",
    "    'association cfr international', 'association cfr', 'produce association cfr'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    min_df=3,\n",
    "    max_df=0.4,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    stop_words=my_stop_words,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus['text_preproc'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f'# Documents: {X.shape[0]}\\n# Terms: {X.shape[1]}\\n')\n",
    "\n",
    "unique_years = sorted(corpus['year'].unique())\n",
    "\n",
    "print(\"--- Top Distinctive Words by Year ---\")\n",
    "\n",
    "top5 = []\n",
    "\n",
    "for year in unique_years:\n",
    "    mask = (corpus['year'] == year).values\n",
    "    X_year = X[mask]\n",
    "    avg_tfidf = X_year.mean(axis=0)\n",
    "    avg_tfidf = np.asarray(avg_tfidf).flatten()\n",
    "    top_indices = avg_tfidf.argsort()[::-1][:5]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    top5.append({\"year\": year, \"n_documents\": mask.sum(), \"top_words\": top_words})\n",
    "    \n",
    "    print(f\"{year} ({mask.sum()} documents): {', '.join(top_words)}\")\n",
    "\n",
    "pd.DataFrame(top5).to_csv('output/top5_words_per_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b21d8",
   "metadata": {},
   "source": [
    "### 4. Exporting the TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21eff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating time-series data...\n",
      "Successfully saved 200 words to output/tfidf_data.json\n"
     ]
    }
   ],
   "source": [
    "TOP_N_WORDS = 200\n",
    "OUTPUT_FILE = \"output/tfidf_data.json\"\n",
    "\n",
    "print(\"Generating time-series data...\")\n",
    "\n",
    "years = sorted(corpus['year'].unique())[2:-1]\n",
    "\n",
    "word_data = {word: [] for word in feature_names}\n",
    "\n",
    "for year in years:\n",
    "    mask = (corpus['year'] == year).values\n",
    "    if mask.sum() > 0:\n",
    "        avg_vector = np.asarray(X[mask].mean(axis=0)).flatten()\n",
    "        \n",
    "        avg_vector = np.round(avg_vector * 1000, 3)\n",
    "        \n",
    "        for i, word in enumerate(feature_names):\n",
    "            word_data[word].append(float(avg_vector[i]))\n",
    "    else:\n",
    "        for word in feature_names:\n",
    "            word_data[word].append(0.0)\n",
    "\n",
    "total_scores = {k: sum(v) for k, v in word_data.items()}\n",
    "sorted_words = sorted(total_scores, key=total_scores.get, reverse=True)[:TOP_N_WORDS]\n",
    "\n",
    "final_data = {\n",
    "    \"years\": list(years),\n",
    "    \"series\": {word: word_data[word] for word in sorted_words}\n",
    "}\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(final_data, f)\n",
    "\n",
    "print(f\"Successfully saved {len(sorted_words)} words to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining_workflow_cfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
